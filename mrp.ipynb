{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d685682-9e9a-4aa0-97f8-6c6b68088044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import altair as alt\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import xarray as xr\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d44c6a-943e-4f5c-be5f-6a3d039739c3",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633cf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"./data/md.csv\"\n",
    "data = pd.read_csv(data_file,index_col=0)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa36767e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the ordinal columns to ordered categories\n",
    "cats = ['I have no confidence', \"I don't have much confidence\", 'No answer', 'Some confidence', 'A lot of confidence']\n",
    "for c in data.columns[9:]:\n",
    "    data[c] = pd.Categorical(data[c], cats, ordered=True)\n",
    "\n",
    "# Convert other columns to categorical\n",
    "for c in data.columns[:9]:\n",
    "    data[c] = pd.Categorical(data[c])#,list(data[c].unique()))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac72377-357d-4904-ba45-9ab1997ae87d",
   "metadata": {},
   "source": [
    "# Basic Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac66abb0",
   "metadata": {},
   "source": [
    "## Define `coords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "focused-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the categorical inputs to integers\n",
    "age_idx, ages = data[\"age_group2\"].factorize(sort=True)\n",
    "education_idx, educations = data[\"education\"].factorize(sort=True)\n",
    "gender_idx, genders = data[\"gender\"].factorize(sort=True)\n",
    "ethnicity_idx, ethnicities = data[\"ethnicity\"].factorize(sort=True)\n",
    "unit_idx, units = data[\"county\"].factorize(sort=True)\n",
    "ur_idx, ur_responses = data[\"ur\"].factorize(sort=True) # Urban/Rural\n",
    "\n",
    "COORDS = {\n",
    "        \"age\": ages,\n",
    "        \"education\": educations,\n",
    "        \"gender\": genders,\n",
    "        \"ethnicity\": ethnicities,\n",
    "        \"unit\": units,\n",
    "        \"ur\": ur_responses,\n",
    "        \n",
    "        # Create a dimension for observations\n",
    "        'obs_idx': list(range(len(data)))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad97e3b",
   "metadata": {},
   "source": [
    "# Basic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c4b7d-c298-4477-8cec-e28f1512ef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we interested in predicting?\n",
    "dependent_col = 'trust2_EU'\n",
    "\n",
    "with pm.Model(coords=COORDS) as model:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   Intercepts\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # intercept (i.e national) latent popularity for each response\n",
    "    intercept = pm.Normal(\"intercept\", sigma=1.5)\n",
    "\n",
    "    # We collect all effects here. It will be of dimension (obs_idx, response)\n",
    "    mu = intercept\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   DATA COMPONENTS\n",
    "    # --------------------------------------------------------\n",
    "    sigma = 0.5\n",
    "\n",
    "    # Declare inputs as Data so we can change them in post-stratification\n",
    "    age_id = pm.Data(\"age_id\", age_idx, dims=\"obs_idx\")\n",
    "    a_age = pm.Normal(\"a_age\", sigma=sigma, dims=\"age\")\n",
    "    mu += a_age[age_id]\n",
    "\n",
    "    education_id = pm.Data(\"education_id\", education_idx, dims=\"obs_idx\")\n",
    "    a_edu = pm.Normal(\"a_edu\", sigma=sigma, dims=\"education\")\n",
    "    mu += a_edu[education_id]\n",
    "\n",
    "    gender_id = pm.Data(\"gender_id\", gender_idx, dims=\"obs_idx\")\n",
    "    a_gender = pm.Normal(\"a_gender\", sigma=sigma, dims=\"gender\")\n",
    "    mu += a_gender[gender_id]\n",
    "\n",
    "    ethnicity_id = pm.Data(\"ethnicity_id\", ethnicity_idx, dims=\"obs_idx\")\n",
    "    a_eth = pm.Normal(\"a_eth\", sigma=sigma, dims=\"ethnicity\")\n",
    "    mu += a_eth[ethnicity_id]\n",
    "    \n",
    "    unit_id = pm.Data(\"unit_id\", unit_idx, dims=\"obs_idx\")\n",
    "    a_unit = pm.Normal(\"a_unit\", sigma=sigma, dims=\"unit\")\n",
    "    mu += a_unit[unit_id]\n",
    "\n",
    "    ur_id = pm.Data(\"ur_id\", ur_idx, dims=\"obs_idx\")\n",
    "    a_ur = pm.Normal(\"a_ur\", sigma=sigma, dims=\"ur\")\n",
    "    mu += a_ur[ur_id]\n",
    "\n",
    "    # We can also add interaction terms:\n",
    "    a_age_edu = pm.Normal(\n",
    "        \"a_age_edu\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"age\", \"education\"),\n",
    "    )\n",
    "    mu += a_age_edu[age_id, education_id]\n",
    "\n",
    "    a_gender_eth = pm.Normal(\n",
    "        \"a_gender_eth\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"gender\", \"ethnicity\"),\n",
    "    )\n",
    "    mu += a_gender_eth[gender_id, ethnicity_id]\n",
    "\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   OBSERVATION MODEL\n",
    "    # -------------------------------------------------------- \n",
    "\n",
    "    multinomial = True\n",
    "\n",
    "    if multinomial == False: # Simple model, but hard to post-stratify\n",
    "\n",
    "        resp_idx, resp_cat = data[dependent_col].factorize(sort=True)\n",
    "        pm.Binomial(\"y\", p=p, N=len(resp_cat)-1, observed=resp_idx)\n",
    "\n",
    "    else: # Effectively the same model as above, but built for post-stratification\n",
    "\n",
    "        # Create a mutable N and set it to 1 for now as we have one observation per person\n",
    "        # This will be changed to the size of the census segment in post-stratification\n",
    "        N = pm.Data(\"N\", np.ones(len(data)), dims=\"obs_idx\")\n",
    "\n",
    "        # Convert the dependent variable to a dummy matrix\n",
    "        obs = pd.get_dummies(data[dependent_col]).astype(int)\n",
    "\n",
    "        # Add a coord for it as well\n",
    "        model.add_coord('response',list(obs.columns))\n",
    "\n",
    "        # Compute the p for each respondent\n",
    "        p = pm.math.invlogit(mu)\n",
    "\n",
    "        # This is hand-made, code in utils.py\n",
    "        # It's the multivariate distribution of answers\n",
    "        # IFF everyone answered according to a binomial distribution\n",
    "        MvBinomial(\"y\", p=p, N=len(obs.columns)-1, \n",
    "            n=N, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "\n",
    "pm.model_to_graphviz(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bbca29",
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    idata = pm.sample(nuts_sampler='nutpie', idata_kwargs={\"log_likelihood\": True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a568c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fit\n",
    "idata.to_netcdf('ord_mrp.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22361dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(idata, var_names=[\"~_offset\",\"~_probs\"], filter_vars=\"regex\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a162e-bb78-497e-80da-8bd367ca3ad2",
   "metadata": {},
   "source": [
    "# Post-stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9ec9e6",
   "metadata": {},
   "source": [
    "### Process census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6da680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdata = pd.read_csv(\"./data/md_census.csv\")\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be82ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop 0 values\n",
    "cdata = cdata[cdata.N > 0]\n",
    "print(len(cdata))\n",
    "\n",
    "# Drop the <15 category\n",
    "cdata = cdata[cdata.age_group != '<15']\n",
    "\n",
    "# Map the age groups to the groups used in the poll\n",
    "cdata['age_group2'] = cdata['age_group'].replace({'15-19': '15-19',\n",
    " '20-24': '20-29',\n",
    " '25-29': '20-29',\n",
    " '30-34': '30-39',\n",
    " '35-39': '30-39',\n",
    " '40-44': '40-49',\n",
    " '45-49': '40-49',\n",
    " '50-54': '50-59',\n",
    " '55-59': '50-59',\n",
    " '60-64': '60+',\n",
    " '65-69': '60+',\n",
    " '70+': '60+'})\n",
    "\n",
    "# Merge over all the columns that are present in both dataframes\n",
    "shared_cols = [c for c in cdata.columns if c in data.columns]\n",
    "cdata = cdata.groupby(shared_cols)['N'].sum().reset_index()\n",
    "\n",
    "cdata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2614a5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the columns that are present in both have matching category values\n",
    "for c in cdata.columns[:-1]:\n",
    "    s1, s2 = set(data[c].unique()), set(cdata[c].unique())\n",
    "    if s1 != s2:\n",
    "        print(c, s1-s2, s2-s1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9765802c-3cc6-485f-9d07-e0ac7bf4da49",
   "metadata": {},
   "source": [
    "### Define new dimensions and coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "551b65f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# I'm surprised pandas does not have this function but I could not find it. \n",
    "def factorize_w_codes(s, codes):\n",
    "    res = s.replace(dict(zip(codes,range(len(codes)))))\n",
    "    if not s.isin(codes).all(): # Throw an exception if all values were not replaced\n",
    "        vals = set(s) - set(codes)\n",
    "        raise Exception(f'Codes for {s.name} do not match all values: {vals}')\n",
    "    return res.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ff5fd9-65b8-45b0-8830-2e827c50a8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poststrat_age_idx = factorize_w_codes(cdata[\"age_group2\"],ages)\n",
    "poststrat_education_idx = factorize_w_codes(cdata[\"education\"],educations)\n",
    "poststrat_gender_idx = factorize_w_codes(cdata[\"gender\"],genders)\n",
    "poststrat_ethnicity_idx = factorize_w_codes(cdata[\"ethnicity\"],ethnicities)\n",
    "poststrat_units_idx = factorize_w_codes(cdata[\"county\"],units)\n",
    "poststrat_ur_idx = factorize_w_codes(cdata[\"ur\"],ur_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b986f529-8265-42ac-a7e8-036b6081adbe",
   "metadata": {},
   "source": [
    "### Condition model on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442014c0-40cb-400b-93d2-c4b653f94779",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {\n",
    "            \"age_id\": poststrat_age_idx,\n",
    "            \"education_id\": poststrat_education_idx,\n",
    "            \"gender_id\": poststrat_gender_idx,\n",
    "            \"ethnicity_id\": poststrat_ethnicity_idx,\n",
    "            \"unit_id\": poststrat_units_idx,\n",
    "            \"ur_id\":poststrat_ur_idx,\n",
    "            # \"obs\": np.zeros(\n",
    "            #     (len(cdata.index), len(y_cols), len(y_options) ),\n",
    "            #     dtype=int,\n",
    "            # ),  # just a placeholder for new data size\n",
    "            \"N\": cdata[\"N\"].to_numpy(),\n",
    "        }\n",
    "\n",
    "with model:\n",
    "    pm.set_data(\n",
    "        coords={\n",
    "            \"obs_idx\": cdata.index,\n",
    "        },\n",
    "        new_data=new_data\n",
    "    )\n",
    "    \n",
    "    print(\"Sampling posterior predictive on census data\")\n",
    "    # Reduce the sample to just 100 draws from 2 chains each\n",
    "    idata_s = idata.sel(draw=slice(0,500),chain=[0,1])\n",
    "    idata_ps = pm.sample_posterior_predictive(\n",
    "        idata_s,\n",
    "        log_likelihood=True,\n",
    "        predictions=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125ebca3",
   "metadata": {},
   "source": [
    "### Create the \"Synthetic Population\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b7b358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = idata_ps.predictions.y\n",
    "\n",
    "# This needed some technical work to not explode memory usage\n",
    "df = pd.DataFrame(\n",
    "    pred.values.reshape(-1), dtype=pd.Int16Dtype(),\n",
    "    columns=['N']\n",
    ").assign(\n",
    "    chain=np.repeat(pred.chain.values, len(pred.draw) * len(pred.obs_idx)*len(pred.response)).astype('int8'),\n",
    "    draw=np.tile(np.repeat(pred.draw.values, len(pred.obs_idx)*len(pred.response)), len(pred.chain)).astype('int16'),\n",
    "    obs_idx=np.tile(pred.obs_idx.values, len(pred.chain) * len(pred.draw)*len(pred.response)).astype('int16'),\n",
    "    response=np.tile(np.arange(len(pred.response)), len(pred.chain) * len(pred.draw)*len(pred.obs_idx)).astype('int8')\n",
    ")\n",
    "df['adraw'] = df['draw'] + df['chain'] * len(pred.draw)\n",
    "#df.memory_usage(deep=True)\n",
    "\n",
    "# Sample a \"synthetic population\" of 1M rows by sampling with replacement with weights based on N\n",
    "df = df.sample(weights='N',n=1000000,replace=True).drop(columns=['N'])\n",
    "df['response'] = pd.Categorical(pred.response.values[df['response']],categories=pred.response.values,ordered=True)\n",
    "\n",
    "# Merge with the census data on obs_idx\n",
    "df = df.merge(cdata,left_on='obs_idx',right_index=True)\n",
    "df = df.drop(columns=['obs_idx','N','chain','draw'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75665c3f",
   "metadata": {},
   "source": [
    "# Explore synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053389f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The synthetic population is now ready to be used for analysis\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09e7046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotnine import *\n",
    "\n",
    "# Draw topline distribution\n",
    "agg = (df.groupby(['response','adraw']).size()/df.groupby('adraw').size()).reset_index(name='p')\n",
    "(ggplot(agg, aes(x='response', y='p')) +\n",
    "    geom_boxplot() +\n",
    "    theme(axis_text_x=element_text(angle=45, hjust=1)) +\n",
    "    labs(x='Response', y='Proportion', title='Distribution of Response Proportions')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "alt.Chart(data).mark_bar().encode(\n",
    "    x=dependent_col,\n",
    "    y='count()',\n",
    ").properties(\n",
    "    width=600,\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aea18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw young urban female highly educated only\n",
    "fquery = \"gender == 'Female' and age_group2.isin(['15-19','20-29']) and education == 'Higher education'\"\n",
    "\n",
    "print(f\"N in original data: {len(data.query(fquery))}\")\n",
    "print(f\"N in synthetic population: {len(df.query(fquery))}\")\n",
    "\n",
    "f_df = df.query(fquery)\n",
    "agg = (f_df.groupby(['response','adraw']).size()/f_df.groupby('adraw').size()).reset_index(name='p')\n",
    "(ggplot(agg, aes(x='response', y='p')) +\n",
    "    geom_boxplot() +\n",
    "    theme(axis_text_x=element_text(angle=45, hjust=1)) +\n",
    "    labs(x='Response', y='Proportion', title='Distribution of Response Proportions')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dedb33",
   "metadata": {},
   "source": [
    "# Other likelihoods for Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4296c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we interested in predicting?\n",
    "dependent_col = 'trust2_EU'\n",
    "model_type = 'stereotype'\n",
    "\n",
    "with pm.Model(coords=COORDS) as model:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   Intercepts\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # intercept (i.e national) latent popularity for each response\n",
    "    intercept = pm.Normal(\"intercept\", sigma=1.5)\n",
    "\n",
    "    # We collect all effects here. It will be of dimension (obs_idx, response)\n",
    "    mu = intercept\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   DATA COMPONENTS\n",
    "    # --------------------------------------------------------\n",
    "    sigma = 0.5\n",
    "\n",
    "    # Declare inputs as Data so we can change them in post-stratification\n",
    "    age_id = pm.Data(\"age_id\", age_idx, dims=\"obs_idx\")\n",
    "    a_age = pm.Normal(\"a_age\", sigma=sigma, dims=\"age\")\n",
    "    mu += a_age[age_id]\n",
    "\n",
    "    education_id = pm.Data(\"education_id\", education_idx, dims=\"obs_idx\")\n",
    "    a_edu = pm.Normal(\"a_edu\", sigma=sigma, dims=\"education\")\n",
    "    mu += a_edu[education_id]\n",
    "\n",
    "    gender_id = pm.Data(\"gender_id\", gender_idx, dims=\"obs_idx\")\n",
    "    a_gender = pm.Normal(\"a_gender\", sigma=sigma, dims=\"gender\")\n",
    "    mu += a_gender[gender_id]\n",
    "\n",
    "    ethnicity_id = pm.Data(\"ethnicity_id\", ethnicity_idx, dims=\"obs_idx\")\n",
    "    a_eth = pm.Normal(\"a_eth\", sigma=sigma, dims=\"ethnicity\")\n",
    "    mu += a_eth[ethnicity_id]\n",
    "    \n",
    "    unit_id = pm.Data(\"unit_id\", unit_idx, dims=\"obs_idx\")\n",
    "    a_unit = pm.Normal(\"a_unit\", sigma=sigma, dims=\"unit\")\n",
    "    mu += a_unit[unit_id]\n",
    "\n",
    "    ur_id = pm.Data(\"ur_id\", ur_idx, dims=\"obs_idx\")\n",
    "    a_ur = pm.Normal(\"a_ur\", sigma=sigma, dims=\"ur\")\n",
    "    mu += a_ur[ur_id]\n",
    "\n",
    "    # We can also add interaction terms:\n",
    "    a_age_edu = pm.Normal(\n",
    "        \"a_age_edu\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"age\", \"education\"),\n",
    "    )\n",
    "    mu += a_age_edu[age_id, education_id]\n",
    "\n",
    "    a_gender_eth = pm.Normal(\n",
    "        \"a_gender_eth\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"gender\", \"ethnicity\"),\n",
    "    )\n",
    "    mu += a_gender_eth[gender_id, ethnicity_id]\n",
    "    pm.Deterministic(\"mu\", mu, dims=\"obs_idx\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   OBSERVATION MODEL\n",
    "    # -------------------------------------------------------- \n",
    "\n",
    "    # Create a mutable N and set it to 1 for now as we have one observation per person\n",
    "    # This will be changed to the size of the census segment in post-stratification\n",
    "    N = pm.Data(\"N\", np.ones(len(data)), dims=\"obs_idx\")\n",
    "\n",
    "    # Convert the dependent variable to a dummy matrix\n",
    "    obs = pd.get_dummies(data[dependent_col]).astype(int)\n",
    "    model.add_coord('response',list(obs.columns))\n",
    "\n",
    "    # Binomial observation model\n",
    "    if model_type == 'binomial':\n",
    "        p = pm.math.invlogit(mu)\n",
    "        MvBinomial(\"y\", p=p, N=len(obs.columns)-1, \n",
    "            n=N, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "            \n",
    "    # Ordered Multinomial observation model (Logit-based)\n",
    "    elif model_type == 'ordered-logit':\n",
    "        # Model for cutpoints\n",
    "        cp_scale = pm.HalfNormal(f\"cutpoint_sd\", sigma=2)\n",
    "        cp_len = len(obs.columns)-1\n",
    "        \n",
    "        cutpoints = pm.ZeroSumNormal(\n",
    "            \"cutpoints\",\n",
    "            sigma=cp_scale,\n",
    "            transform=pm.distributions.transforms.ordered,\n",
    "            initval=np.linspace(-1,1,cp_len),\n",
    "            shape=cp_len\n",
    "        )\n",
    "\n",
    "        # Ordered Multinomial observation model (Logit-based)\n",
    "        pm.OrderedMultinomial(\n",
    "            \"y\",\n",
    "            eta=mu,\n",
    "            cutpoints=cutpoints,\n",
    "            n=N,\n",
    "            observed=obs.values,\n",
    "            dims=(\"obs_idx\", \"response\"),\n",
    "        )\n",
    "\n",
    "    # Stereotype regression\n",
    "    elif model_type == 'stereotype':\n",
    "\n",
    "        # Generate phi values: first is 0, last is 1 and the inbetween is increasing\n",
    "        n_ordered = len(obs.columns)\n",
    "        phi_delta = pm.Dirichlet(f'phi_diffs', [1.0]*(n_ordered-1) )\n",
    "        phi = pt.concatenate([[0], pt.cumsum(phi_delta)])\n",
    "\n",
    "        # Create baseline values\n",
    "        b = (pm.Normal(\"stereotype_intercept\", size=n_ordered, \n",
    "                                transform=pm.distributions.transforms.ZeroSumTransform([-1])))\n",
    "\n",
    "        # Log_odds = baseline + phi*mu\n",
    "        log_odds = b[None,:] + phi[None,:]*mu[:,None]\n",
    "        \n",
    "        # Run them through softmax\n",
    "        probs = pm.math.softmax(log_odds, axis=-1)\n",
    "\n",
    "        pm.Multinomial(\"y\", p=probs, n=N, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "\n",
    "# Do the same thing as above for post-stratification. Code in utils.py\n",
    "df_om, idata_om = poststratify(model,cdata,new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ffb7dc",
   "metadata": {},
   "source": [
    "# Hierarchical block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b78433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hierarchical building block for the model    \n",
    "def hierarchical_zsn(name: str, dim: str, sigma: float = 0.2):\n",
    "\n",
    "    scale = pm.HalfNormal(f\"{name}_sd\", sigma=sigma)\n",
    "    offset = pm.ZeroSumNormal(f\"{name}_offset\", dims=dim)\n",
    "    \n",
    "    # Effect = scale * offset\n",
    "    return pm.Deterministic(f\"{name}\", scale * offset, dims=(dim))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd792413",
   "metadata": {},
   "source": [
    "# Improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0869b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we interested in predicting?\n",
    "dependent_col = 'trust2_EU'\n",
    "model_type = 'binomial'\n",
    "\n",
    "with pm.Model(coords=COORDS) as model:\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   Intercepts\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # intercept (i.e national) latent popularity for each response\n",
    "    intercept = pm.Normal(\"intercept\", sigma=2)\n",
    "\n",
    "    # We collect all effects here. It will be of dimension (obs_idx, response)\n",
    "    mu = intercept\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   DATA COMPONENTS\n",
    "    # --------------------------------------------------------\n",
    "    sigma = 0.5\n",
    "\n",
    "    # Declare inputs as Data so we can change them in post-stratification\n",
    "    age_id = pm.Data(\"age_id\", age_idx, dims=\"obs_idx\")\n",
    "    a_age = hierarchical_zsn(\"a_age\", \"age\", sigma=sigma)\n",
    "    mu += a_age[age_id]\n",
    "\n",
    "    education_id = pm.Data(\"education_id\", education_idx, dims=\"obs_idx\")\n",
    "    a_edu = hierarchical_zsn(\"a_edu\", \"education\", sigma=sigma)\n",
    "    mu += a_edu[education_id]\n",
    "\n",
    "    gender_id = pm.Data(\"gender_id\", gender_idx, dims=\"obs_idx\")\n",
    "    a_gender = hierarchical_zsn(\"a_gender\", \"gender\", sigma=sigma)\n",
    "    mu += a_gender[gender_id]\n",
    "\n",
    "    ethnicity_id = pm.Data(\"ethnicity_id\", ethnicity_idx, dims=\"obs_idx\")\n",
    "    a_eth = hierarchical_zsn(\"a_eth\", \"ethnicity\", sigma=sigma)\n",
    "    mu += a_eth[ethnicity_id]\n",
    "    \n",
    "    unit_id = pm.Data(\"unit_id\", unit_idx, dims=\"obs_idx\")\n",
    "    a_unit = hierarchical_zsn(\"a_unit\", \"unit\", sigma=sigma)\n",
    "    mu += a_unit[unit_id]\n",
    "\n",
    "    ur_id = pm.Data(\"ur_id\", ur_idx, dims=\"obs_idx\")\n",
    "    a_ur = hierarchical_zsn(\"a_ur\", \"ur\", sigma=sigma)\n",
    "    mu += a_ur[ur_id]\n",
    "\n",
    "\n",
    "    # We can also add interaction terms:\n",
    "    a_age_edu = pm.ZeroSumNormal(\n",
    "        \"a_age_edu\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"age\", \"education\"),\n",
    "        n_zerosum_axes=2,\n",
    "    )\n",
    "    mu += a_age_edu[age_id, education_id]\n",
    "\n",
    "    a_gender_eth = pm.ZeroSumNormal(\n",
    "        \"a_gender_eth\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"gender\", \"ethnicity\"),\n",
    "        n_zerosum_axes=2,\n",
    "    )\n",
    "    mu += a_gender_eth[gender_id, ethnicity_id]\n",
    "\n",
    "    pm.Deterministic(\"mu\", mu, dims=\"obs_idx\")\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   OBSERVATION MODEL\n",
    "    # -------------------------------------------------------- \n",
    "\n",
    "    # Create a mutable N and set it to 1 for now as we have one observation per person\n",
    "    # This will be changed to the size of the census segment in post-stratification\n",
    "    N = pm.Data(\"N\", np.ones(len(data)), dims=\"obs_idx\")\n",
    "\n",
    "    # Convert the dependent variable to a dummy matrix\n",
    "    obs = pd.get_dummies(data[dependent_col]).astype(int)\n",
    "    model.add_coord('response',list(obs.columns))\n",
    "\n",
    "    # Binomial observation model\n",
    "    if model_type == 'binomial':\n",
    "        p = pm.math.invlogit(mu)\n",
    "        MvBinomial(\"y\", p=p, N=len(obs.columns)-1, \n",
    "            n=N, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "            \n",
    "    # Ordered Multinomial observation model (Logit-based)\n",
    "    elif model_type == 'ordered-logit':\n",
    "        # Model for cutpoints\n",
    "        cp_scale = pm.HalfNormal(f\"cutpoint_sd\", sigma=2)\n",
    "        cp_len = len(obs.columns)-1\n",
    "        \n",
    "        cutpoints = pm.ZeroSumNormal(\n",
    "            \"cutpoints\",\n",
    "            sigma=cp_scale,\n",
    "            transform=pm.distributions.transforms.ordered,\n",
    "            initval=np.linspace(-1,1,cp_len),\n",
    "            shape=cp_len\n",
    "        )\n",
    "\n",
    "        # Ordered Multinomial observation model (Logit-based)\n",
    "        pm.OrderedMultinomial(\n",
    "            \"y\",\n",
    "            eta=mu,\n",
    "            cutpoints=cutpoints,\n",
    "            n=N,\n",
    "            observed=obs.values,\n",
    "            dims=(\"obs_idx\", \"response\"),\n",
    "        )\n",
    "\n",
    "    # Stereotype regression\n",
    "    elif model_type == 'stereotype':\n",
    "\n",
    "        # Generate phi values: first is 0, last is 1 and the inbetween is increasing\n",
    "        n_ordered = len(obs.columns)\n",
    "        phi_delta = pm.Dirichlet(f'phi_diffs', [1.0]*(n_ordered-1) )\n",
    "        phi = pt.concatenate([[0], pt.cumsum(phi_delta)])\n",
    "\n",
    "        # Create baseline values\n",
    "        b = (pm.Normal(\"stereotype_intercept\", size=n_ordered, \n",
    "                                transform=pm.distributions.transforms.ZeroSumTransform([-1])))\n",
    "\n",
    "        # Log_odds = baseline + phi*mu\n",
    "        log_odds = b[None,:] + phi[None,:]*mu[:,None]\n",
    "        \n",
    "        # Run them through softmax\n",
    "        probs = pm.math.softmax(log_odds, axis=-1)\n",
    "\n",
    "        pm.Multinomial(\"y\", p=probs, n=N, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "\n",
    "# Do the same thing as above for post-stratification. Code in utils.py\n",
    "df2, idata2 = poststratify(model,cdata,new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7c294b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw young urban female highly educated only\n",
    "fquery = \"gender == 'Female' and age_group2.isin(['15-19','20-29']) and education == 'Higher education'\"\n",
    "\n",
    "print(f\"N in original data: {len(data.query(fquery))}\")\n",
    "print(f\"N in synthetic population: {len(df2.query(fquery))}\")\n",
    "\n",
    "f_df = df2.query(fquery)\n",
    "agg = (f_df.groupby(['response','adraw']).size()/f_df.groupby('adraw').size()).reset_index(name='p')\n",
    "(ggplot(agg, aes(x='response', y='p')) +\n",
    "    geom_boxplot() +\n",
    "    theme(axis_text_x=element_text(angle=45, hjust=1)) +\n",
    "    labs(x='Response', y='Proportion', title='Distribution of Response Proportions')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f75300",
   "metadata": {},
   "source": [
    "# Regular (non-ordered) categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d56ce4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A hierarchical building block for the model    \n",
    "def hierarchical_zsn_2d(name: str, dim: str, odim = \"question\", sigma: float = 0.2):\n",
    "\n",
    "    scale = pm.HalfNormal(f\"{name}_sd\", sigma=sigma, dims=odim) # Separate sd per output category\n",
    "    offset = pm.ZeroSumNormal(f\"{name}_offset\", dims=(dim,odim), n_zerosum_axes=2)\n",
    "\n",
    "    return pm.Deterministic(f\"{name}\", scale[None,:] * offset, dims=(dim,odim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf29dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What are we interested in predicting?\n",
    "dependent_col = 'trust2_EU' # You CAN do this with ordered, but it will be noisier\n",
    "\n",
    "with pm.Model(coords=COORDS) as model:\n",
    "\n",
    "    # Convert the dependent variable to a dummy matrix\n",
    "    obs = pd.get_dummies(data[dependent_col]).astype(int)\n",
    "    model.add_coord('response',list(obs.columns))\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   Intercepts\n",
    "    # --------------------------------------------------------\n",
    "\n",
    "    # intercept (i.e national) latent popularity for each response\n",
    "    intercept_sd = pm.HalfNormal(\"intercept_sd\", 2)\n",
    "    intercept = pm.Normal(\"intercept\", sigma=intercept_sd, dims=\"response\")\n",
    "\n",
    "    # We collect all effects here. It will be of dimension (obs_idx, response)\n",
    "    mu = intercept[None,:]\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   DATA COMPONENTS\n",
    "    # --------------------------------------------------------\n",
    "    sigma = 0.5\n",
    "\n",
    "    # Declare inputs as Data so we can change them in post-stratification\n",
    "    age_id = pm.Data(\"age_id\", age_idx, dims=\"obs_idx\")\n",
    "    a_age = hierarchical_zsn_2d(\"a_age\", \"age\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_age[age_id]\n",
    "\n",
    "    education_id = pm.Data(\"education_id\", education_idx, dims=\"obs_idx\")\n",
    "    a_edu = hierarchical_zsn_2d(\"a_edu\", \"education\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_edu[education_id]\n",
    "\n",
    "    gender_id = pm.Data(\"gender_id\", gender_idx, dims=\"obs_idx\")\n",
    "    a_gender = hierarchical_zsn_2d(\"a_gender\", \"gender\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_gender[gender_id]\n",
    "\n",
    "    ethnicity_id = pm.Data(\"ethnicity_id\", ethnicity_idx, dims=\"obs_idx\")\n",
    "    a_eth = hierarchical_zsn_2d(\"a_eth\", \"ethnicity\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_eth[ethnicity_id]\n",
    "    \n",
    "    unit_id = pm.Data(\"unit_id\", unit_idx, dims=\"obs_idx\")\n",
    "    a_unit = hierarchical_zsn_2d(\"a_unit\", \"unit\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_unit[unit_id]\n",
    "\n",
    "    ur_id = pm.Data(\"ur_id\", ur_idx, dims=\"obs_idx\")\n",
    "    a_ur = hierarchical_zsn_2d(\"a_ur\", \"ur\", odim=\"response\", sigma=sigma)\n",
    "    mu += a_ur[ur_id]\n",
    "\n",
    "    # We can also add interaction terms:\n",
    "    a_age_edu = pm.ZeroSumNormal(\n",
    "        \"a_age_edu\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"age\", \"education\", \"response\"),\n",
    "        n_zerosum_axes=3,\n",
    "    )\n",
    "    mu += a_age_edu[age_id, education_id]\n",
    "\n",
    "    a_gender_eth = pm.ZeroSumNormal(\n",
    "        \"a_gender_eth\",\n",
    "        sigma=0.5*sigma,\n",
    "        dims=(\"gender\", \"ethnicity\", \"response\"),\n",
    "        n_zerosum_axes=3,\n",
    "    )\n",
    "    mu += a_gender_eth[gender_id, ethnicity_id]\n",
    "\n",
    "    pm.Deterministic(\"mu\", mu, dims=(\"obs_idx\",\"response\"))\n",
    "\n",
    "    # Create a mutable N and set it to 1 for now as we have one observation per person\n",
    "    # This will be changed to the size of the census segment in post-stratification\n",
    "    N = pm.Data(\"N\", np.ones(len(data)), dims=\"obs_idx\")\n",
    "\n",
    "    # Softmax: pm version fails for some reason\n",
    "    # p = pm.math.softmax(mu)\n",
    "    p = pt.exp(mu)\n",
    "    p/= pt.sum(p, axis=-1, keepdims=True)\n",
    "    \n",
    "    pm.Multinomial(\"y\", n=N, p=p, observed=obs.values, dims=(\"obs_idx\", \"response\"))\n",
    "\n",
    "df3, idata3 = poststratify(model,cdata,new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
