{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "\n",
    "az.style.use([\"arviz-darkgrid\"])  #, \"arviz-doc\"])\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "with pm.Model() as m:\n",
    "    # y ~ 2 * x\n",
    "    x = pm.MutableData(\"x\", [-2, -1, 0, 1, 2])\n",
    "    y_obs = [-4, -1.7, -0.1, 1.8, 4.1]\n",
    "\n",
    "    beta = pm.Normal(\"beta\")\n",
    "    y = pm.Normal(\"y\", mu=beta * x, sigma=0.1, shape=x.shape, observed=y_obs)\n",
    "\n",
    "    idata = pm.sample()\n",
    "\n",
    "pm.model_to_graphviz(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    pp = pm.sample_posterior_predictive(idata)\n",
    "\n",
    "pm.plot_ppc(pp);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have ran the inference and learned the model parameters we can simply change the data with ``pm.set_data()`` and condition the model on new predictor values. This is how we post-stratified our estimates with an MRP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with m:\n",
    "    # Make predictions conditioned on new Xs\n",
    "    pm.set_data({\"x\": [-1, 3, 5]})\n",
    "    pp = pm.sample_posterior_predictive(idata, predictions=True)\n",
    "\n",
    "pm.plot_posterior(pp, group=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify a *new model* with new predictor values while sampling from the posterior of the different model. The posterior samples for the ``beta`` variable contained in the previous posterior are used when making predictions in this new (purely predictive) model, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pred_m:\n",
    "    # Only x changes\n",
    "    x = np.array([-1, 0, 1])\n",
    "\n",
    "    beta = pm.Normal(\"beta\")\n",
    "    y_pred = pm.Normal(\"y_pred\", mu=beta * x, sigma=0.1, shape=x.shape)\n",
    "\n",
    "    pp = pm.sample_posterior_predictive(\n",
    "        idata, \n",
    "        var_names=[\"y_pred\"], # here we define y_pred as an unobserved random variable, as it is prediction and not observation\n",
    "        predictions=True, \n",
    "    )\n",
    "    \n",
    "pm.plot_posterior(pp, group=\"predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is neat and allows us to generate out-of-sample predictions that can certainly be very useful. However, we can use this for more complex and interesting purposes. For example, we can build a new model with a different likelihood and still sample from the posterior of a previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pred_t_m:\n",
    "    # Using the same x as in the last example\n",
    "    x = np.array([-1, 0, 1])\n",
    "\n",
    "    beta = pm.Normal(\"beta\")\n",
    "\n",
    "    # Only the likelihood distribution changes\n",
    "    y_t = pm.StudentT(\"y_pred_t\", nu=4, mu=beta * x, sigma=0.1)\n",
    "\n",
    "    pp_t = pm.sample_posterior_predictive(\n",
    "        idata, \n",
    "        var_names=[\"y_pred_t\"], \n",
    "        predictions=True, \n",
    "    )\n",
    "\n",
    "pm.plot_posterior(pp, group=\"predictions\")\n",
    "pm.plot_posterior(pp_t, group=\"predictions\", color=\"C1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the \"transferred variables\" do not even have the same priors as in the original model. To illustrate this, let's change the ``pm.Beta()`` prior in the original model to ``pm.Flat()``in the new one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as pred_bern_m:\n",
    "    x = np.linspace(-1, 1, 25)\n",
    "\n",
    "    beta = pm.Flat(\"beta\")\n",
    "\n",
    "    # We again change the functional form of the model\n",
    "    # Instead of a linear Gaussian we Have a logistic Bernoulli model\n",
    "    p = pm.Deterministic(\"p\", pm.math.sigmoid(beta * x)) # pm.math.sigmoid is the logistic function that yields us Bernoulli draws\n",
    "    y = pm.Bernoulli(\"y\", p=p)\n",
    "\n",
    "    pp = pm.sample_posterior_predictive(\n",
    "        idata, \n",
    "        var_names=[\"p\", \"y\"], \n",
    "        predictions=True, \n",
    "    )\n",
    "\n",
    "# a helper function to add jitter to the data\n",
    "def jitter(x, rng):\n",
    "    return rng.normal(x, 0.02)\n",
    "\n",
    "# plot the posterior predictive distribution\n",
    "x = np.linspace(-1, 1, 25)\n",
    "for i in range(25):\n",
    "    p = pp.predictions[\"p\"].sel(chain=0, draw=i)\n",
    "    y = pp.predictions[\"y\"].sel(chain=0, draw=i)\n",
    "\n",
    "    plt.plot(x, p, color=\"C0\", alpha=.1)\n",
    "    plt.scatter(jitter(x, rng), jitter(y, rng), s=10, color=\"k\", alpha=.1)\n",
    "\n",
    "plt.plot([], [], color=\"C0\", label=\"p\")\n",
    "plt.scatter([], [], color=\"k\", label=\"y + jitter\")\n",
    "plt.legend(loc=(1.03, 0.75));\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also do this with hierarchical models and sample estimates for groups that are not represented in the data. In that case we will be sampling values from the prior.\n",
    "\n",
    "To illustrate this let's take the famous [eight schools model](https://www.jstor.org/stable/1164617) illustrating the effectiveness of SAT coaching programs conducted in parallel at eight schools as a baseline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([28, 8, -3, 7, -1, 1, 18, 12])\n",
    "sigma = np.array([15, 10, 16, 11, 9, 11, 10, 18])\n",
    "J = len(y)\n",
    "\n",
    "with pm.Model() as eight_schools:\n",
    "    eta = pm.Normal(\"eta\", 0, 1, shape=J)\n",
    "\n",
    "    # Hierarchical mean and SD\n",
    "    mu = pm.Normal(\"mu\", 0, sigma=10)\n",
    "    tau = pm.HalfNormal(\"tau\", 10)\n",
    "\n",
    "    # Non-centered parameterization of random effect\n",
    "    theta = pm.Deterministic(\"theta\", mu + tau * eta)\n",
    "\n",
    "    pm.Normal(\"y\", theta, sigma=sigma, observed=y)\n",
    "\n",
    "    idata = pm.sample(2000, target_accept=0.9)\n",
    "\n",
    "pm.plot_posterior(idata, var_names=[\"eta\"], ref_val=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we add two more schools that were not observed.\n",
    "\n",
    "We set the model up in a way that allows us to get posterior predictive draws for all 10 schools. For this we create two vectors of variables separately, ``eta`` and ``eta_new`` and concatenate them. The ``sample_posterior_predictive`` function will reuse the ``InferenceData`` draws for ``eta`` and take new draws for ``eta_new``. The predictions for new schools are informed by the group-level variables ``mu`` and ``tau``, which were estimated via sampling of the original subset of 8 schools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ten_schools:\n",
    "    # Priors for schools 9 and 10\n",
    "    # We assume that the mean of school 10 is expected to be one std above the mean\n",
    "    # and have a relatively low measurement error\n",
    "    eta_new = pm.Normal(\"eta_new\", mu=[0, 1.0], sigma=1)\n",
    "    sigma_new = pm.Uniform(\"sigma_new\", lower=[10, 5], upper=[20, 7])\n",
    "\n",
    "    # These are unchanged\n",
    "    eta = pm.Normal(\"eta\", 0, 1, shape=J)\n",
    "    mu = pm.Normal(\"mu\", 0, sigma=10)\n",
    "    tau = pm.HalfNormal(\"tau\", 10)\n",
    "\n",
    "    # We concatenate the variables from the old and new groups\n",
    "    theta = pm.Deterministic(\"theta\", mu + tau * pm.math.concatenate([eta, eta_new]))\n",
    "    pm.Normal(\"y\", theta, sigma=pm.math.concatenate([sigma, sigma_new]))\n",
    "\n",
    "    pp = pm.sample_posterior_predictive(idata, var_names=[\"y\"])\n",
    "\n",
    "pm.summary(pp, group=\"posterior_predictive\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import seaborn as sns\n",
    "\n",
    "pps = az.extract(pp, group=\"posterior_predictive\")\n",
    "\n",
    "_, ax = plt.subplots(5, 2, figsize=(8, 8), sharex=True, sharey=True)\n",
    "for i, axi in enumerate(ax.ravel()):\n",
    "    sns.kdeplot(pps[\"y\"][i], fill=True, ax=axi, color=\"C0\" if i < 8 else \"C1\")\n",
    "    axi.axvline(0, ls=\"--\", c=\"k\")\n",
    "    axi.set_title(f\"School {i}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead -- or in addition to -- predicting model outcomes we may be also interested in predicting latent variables. We can create an example with censored data, using PyMC-s ``pm.Censored``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_censored_obs = [4.3, 5.0, 5.0, 3.2, 0.7, 5.0]\n",
    "\n",
    "with pm.Model() as censored_m:\n",
    "    mu = pm.Normal(\"mu\")\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    x = pm.Normal.dist(mu, sigma)\n",
    "    x_censored = pm.Censored(\n",
    "        \"x_censored\", \n",
    "        dist=x, \n",
    "        lower=None, \n",
    "        upper=5.0, \n",
    "        observed=x_censored_obs,\n",
    "    )\n",
    "\n",
    "    idata = pm.sample()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we take the same model and add a new variable that is not censored. We can then sample from the posterior predictive distribution for this new latent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as uncensored_m:\n",
    "    mu = pm.Normal(\"mu\")\n",
    "    sigma = pm.HalfNormal(\"sigma\")\n",
    "\n",
    "    x = pm.Normal.dist(mu, sigma)\n",
    "    x_censored = pm.Censored(\"x_censored\", dist=x, lower=None, upper=5.0)\n",
    "\n",
    "    # This uncensored variable is new\n",
    "    x_uncensored = pm.Normal(\"x_uncensored\", mu, sigma)\n",
    "\n",
    "    pp = pm.sample_posterior_predictive(\n",
    "        idata,\n",
    "        var_names=[\"x_censored\", \"x_uncensored\"],\n",
    "        predictions=True,\n",
    "    )\n",
    "\n",
    "az.plot_posterior(pp, group=\"predictions\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WHAT IF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can once more return to the eight schools example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymc import do\n",
    "\n",
    "pm.model_to_graphviz(eight_schools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_effect = do(eight_schools, {\"eta\": np.zeros(J)}) \n",
    "\n",
    "with no_effect:\n",
    "    pp = pm.sample_posterior_predictive(idata, var_names=[\"eta\", \"y\"], predictions=True)\n",
    "\n",
    "pp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp.predictions.eta.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pps = az.extract(pp, group=\"predictions\")\n",
    "\n",
    "_, ax = plt.subplots(4, 2, figsize=(6, 6), sharex=True, sharey=True)\n",
    "for i, axi in enumerate(ax.ravel()):\n",
    "    sns.kdeplot(pps[\"y\"][i], fill=True, ax=axi, color=\"C0\")\n",
    "    axi.axvline(0, ls=\"--\", c=\"k\")\n",
    "    axi.set_title(f\"School {i}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "salk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
